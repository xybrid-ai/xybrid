//! Cloud LLM API Demo
//!
//! This example demonstrates using the LLM client to call
//! OpenAI and Anthropic APIs.
//!
//! Prerequisites:
//! - Set OPENAI_API_KEY environment variable for OpenAI
//! - Set ANTHROPIC_API_KEY environment variable for Anthropic
//!
//! Usage:
//!   cargo run --example cloud_llm_demo [openai|anthropic] [prompt]

use std::time::Instant;
use xybrid_core::llm::{Llm, LlmConfig, CompletionRequest};

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let total_start = Instant::now();

    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("  Cloud LLM API Demo");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();

    // Parse command line arguments
    let args: Vec<String> = std::env::args().collect();
    let provider_arg = args.get(1).map(|s| s.as_str()).unwrap_or("openai");
    let prompt = args.get(2).cloned().unwrap_or_else(|| {
        "What is the capital of France? Answer in one sentence.".to_string()
    });

    // Determine provider
    let provider = provider_arg;

    println!("ğŸ¤– Provider: {}", provider);
    println!("ğŸ“ Prompt: \"{}\"", prompt);
    println!();

    // Check for API key based on provider
    let env_var = match provider {
        "anthropic" => "ANTHROPIC_API_KEY",
        _ => "OPENAI_API_KEY",
    };

    if std::env::var(env_var).is_err() {
        eprintln!("âŒ {} not set", env_var);
        eprintln!();
        eprintln!("Please set the environment variable:");
        eprintln!("  export {}=your-api-key", env_var);
        return Ok(());
    }

    // Create client with direct provider access
    let client_start = Instant::now();
    let config = LlmConfig::direct(provider);
    let client = Llm::with_config(config)?;
    let client_latency = client_start.elapsed();
    println!("âœ… Client created successfully");
    println!("   â±ï¸  Client init: {:.2}ms", client_latency.as_secs_f64() * 1000.0);
    println!();

    // Build request
    let request = CompletionRequest::new(&prompt)
        .with_system("You are a helpful assistant. Be concise.")
        .with_max_tokens(100)
        .with_temperature(0.7);

    println!("ğŸ”„ Sending request...");

    // Send request with timing
    let request_start = Instant::now();
    let response = client.complete(request)?;
    let request_latency = request_start.elapsed();

    println!();
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("  Response");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();
    println!("ğŸ“¤ Model: {}", response.model);
    println!();
    println!("ğŸ’¬ Response:");
    println!("   {}", response.text);
    println!();

    if let Some(usage) = &response.usage {
        println!("ğŸ“Š Token Usage:");
        println!("   Prompt:     {} tokens", usage.prompt_tokens);
        println!("   Completion: {} tokens", usage.completion_tokens);
        println!("   Total:      {} tokens", usage.total_tokens);
        println!();
    }

    if let Some(finish_reason) = &response.finish_reason {
        println!("ğŸ Finish Reason: {}", finish_reason);
        if finish_reason == "length" {
            println!("   âš ï¸  Response was truncated (hit max_tokens)");
        }
    }

    let total_latency = total_start.elapsed();

    println!();
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("  Latency Summary");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();
    println!("â±ï¸  Client initialization: {:>8.2}ms", client_latency.as_secs_f64() * 1000.0);
    println!("â±ï¸  API request:           {:>8.2}ms", request_latency.as_secs_f64() * 1000.0);
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    println!("â±ï¸  Total:                 {:>8.2}ms", total_latency.as_secs_f64() * 1000.0);
    println!();

    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("  Demo Complete!");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();
    println!("ğŸ¯ Features demonstrated:");
    println!("   â€¢ Provider abstraction (OpenAI/Anthropic)");
    println!("   â€¢ System prompts");
    println!("   â€¢ Temperature control");
    println!("   â€¢ Token usage tracking");
    println!("   â€¢ Latency measurement");
    println!();

    Ok(())
}
