[package]
name = "xybrid-core"
version = "0.0.1"
edition = "2021"

[lib]
name = "xybrid_core"
path = "src/lib.rs"

[dependencies]
serde = { version = "1.0", features = ["derive"] }
serde_yaml = "0.9"
serde_json = "1.0"
anyhow = "1.0"
dirs = "5.0"
tar = "0.4"
zstd = "0.13"
sha2 = "0.10"
thiserror = "1.0"
uuid = { version = "1.0", features = ["v4"] }
chrono = "0.4"
bincode = "1.3"
# ureq is configured per-platform below for Android compatibility
# Android needs rustls with ring (aws-lc-rs uses getentropy unavailable on API 24)
url = "2.5"
tokio = { version = "1.0", features = ["rt", "rt-multi-thread", "sync"] }
tempfile = "3.8"
log = "0.4"  # Standard logging facade

# System information for device detection (v0.0.7)
sysinfo = "0.32"

# ONNX Runtime for model inference
# Note: Using ort crate for ONNX Runtime bindings
# For macOS/iOS: Uses CoreML/Metal backend (enable ort-coreml feature)
# For Android: Uses NNAPI backend (requires ort-dynamic + AAR)
# For desktop: Uses CPU backend
# Note: Using default-features = false for cross-platform compatibility.
# - For Android: Use ort-dynamic feature (loads .so at runtime)
# - For other platforms: Use ort-download feature to download prebuilt binaries
# - For CoreML acceleration: Use ort-coreml feature (macOS/iOS only)
ort = { version = "2.0.0-rc.11", default-features = false, features = ["ndarray", "std"] }
ndarray = "0.17"  # Used by ort for tensor operations (must match ort's ndarray version)
base64 = "0.21"  # For BPE token decoding
mel_spec = "0.2"  # For mel spectrogram computation (audio preprocessing)
num-complex = "0.4"  # Required by mel_spec for Complex numbers
rustfft = "6.2"  # For custom Whisper-compatible FFT (Slaney mel scale)
image = "0.25"  # For image resizing and processing
tokenizers = "0.19"  # For text tokenization (BERT, GPT, etc.)
hound = "3.5"  # For WAV file reading (audio preprocessing)
cmudict-fast = "0.8"  # For CMU pronouncing dictionary (TTS phonemization)
lazy_static = "1.4"  # For static CMU dictionary initialization
unicode-normalization = "0.1"  # For NFC normalization of IPA phonemes

# Kokoro TTS dependencies (v0.0.10)
# Note: espeak-ng phonemization now uses system command (no library linking required)
ndarray-npy = "0.10"  # For loading voice embeddings from npz files (must match ndarray version)
regex = "1.10"  # For text normalization patterns

# Candle - Pure Rust ML framework (optional, for Whisper ASR)
# Enable with: cargo build --features candle
candle-core = { version = "0.8", optional = true }
candle-nn = { version = "0.8", optional = true }
candle-transformers = { version = "0.8", optional = true }
safetensors = { version = "0.4", optional = true }
hf-hub = { version = "0.3", optional = true }  # For downloading models from HuggingFace
byteorder = { version = "1.5", optional = true }  # For reading mel filter bytes
num-traits = { version = "0.2", optional = true }  # Required by candle audio

# Mistral.rs - Pure Rust LLM inference (optional, for local LLMs)
# Enable with: cargo build --features llm-mistral
# For GPU: cargo build --features llm-mistral-metal (macOS) or llm-mistral-cuda (NVIDIA)
# Xcode 26 Metal fix merged: https://github.com/EricLBuehler/mistral.rs/pull/1846
mistralrs = { git = "https://github.com/EricLBuehler/mistral.rs.git", branch = "master", optional = true }

[features]
default = ["ort-download"]

# ONNX Runtime
ort-download = ["ort/download-binaries", "ort/tls-native"]  # Desktop/macOS/iOS: download prebuilt binaries
ort-dynamic = ["ort/load-dynamic"]                          # Android: load .so at runtime from AAR
ort-coreml = ["ort/coreml"]                                 # Apple Neural Engine acceleration

# Candle (Pure Rust ML)
candle = ["dep:candle-core", "dep:candle-nn", "dep:candle-transformers", "dep:safetensors", "dep:hf-hub", "dep:byteorder", "dep:num-traits"]
candle-metal = ["candle", "candle-core/metal", "candle-nn/metal"]
candle-cuda = ["candle", "candle-core/cuda"]

# Local LLM inference
llm-mistral = ["dep:mistralrs"]                             # mistral.rs (CPU)
llm-mistral-metal = ["llm-mistral", "mistralrs/metal"]      # mistral.rs + Metal (macOS/iOS)
llm-mistral-cuda = ["llm-mistral", "mistralrs/cuda"]        # mistral.rs + CUDA (NVIDIA)

# llama.cpp backend (Android-compatible, runtime SIMD detection)
# Note: This is a marker feature - it gates build.rs (compiles llama.cpp via cmake)
# and source code (#[cfg(feature = "llm-llamacpp")] blocks), not external dependencies.
# Requires: vendor/llama.cpp directory with cloned llama.cpp source.
llm-llamacpp = []

[build-dependencies]
# For compiling llama.cpp C/C++ source (used by build.rs when llm-llamacpp is enabled)
cc = "1.0"
cmake = "0.1"

[dev-dependencies]
tempfile = "3.8"
xybrid-sdk = { path = "../sdk" }
dirs = "5.0"
filetime = "0.2"
hound = "3.5"  # WAV file reading for tests
walkdir = "2.4"  # For directory traversal in tests
ureq = { version = "2.8", default-features = false, features = ["json", "tls"] }  # For HTTP in tests
criterion = { version = "0.5", features = ["html_reports"] }  # Benchmarking framework

[[bench]]
name = "inference_backends"
harness = false

# Platform-specific ureq dependencies
# Non-Android: Use default ureq with TLS
[target.'cfg(not(target_os = "android"))'.dependencies]
ureq = { version = "2.8", default-features = false, features = ["json", "tls"] }

# Android: Use ureq with explicit rustls/ring (aws-lc-rs uses getentropy unavailable on API 24)
[target.'cfg(target_os = "android")'.dependencies]
ureq = { version = "2.8", default-features = false, features = ["json", "tls"] }
# Force rustls to use ring crypto backend instead of aws-lc-rs
rustls = { version = "0.23", default-features = false, features = ["ring", "std", "tls12"] }
