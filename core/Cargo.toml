[package]
name = "xybrid-core"
version = "0.0.1"
edition = "2021"

[lib]
name = "xybrid_core"
path = "src/lib.rs"

[dependencies]
serde = { version = "1.0", features = ["derive"] }
serde_yaml = "0.9"
serde_json = "1.0"
anyhow = "1.0"
dirs = "5.0"
tar = "0.4"
zstd = "0.13"
sha2 = "0.10"
thiserror = "1.0"
uuid = { version = "1.0", features = ["v4"] }
chrono = "0.4"
bincode = "1.3"
# ureq is configured per-platform below for Android compatibility
# Android needs rustls with ring (aws-lc-rs uses getentropy unavailable on API 24)
url = "2.5"
tokio = { version = "1.0", features = ["rt", "rt-multi-thread", "sync"] }
tempfile = "3.8"
log = "0.4"  # Standard logging facade

# System information for device detection (v0.0.7)
sysinfo = "0.32"

# ONNX Runtime for model inference
# Note: Using ort crate for ONNX Runtime bindings
# For macOS/iOS: Uses CoreML/Metal backend (enable coreml-ep feature)
# For Android: Uses NNAPI backend (requires load-dynamic + AAR)
# For desktop: Uses CPU backend
# Note: Using default-features = false for cross-platform compatibility.
# - For Android: Use ort-load-dynamic feature (loads .so at runtime)
# - For other platforms: Use ort-download feature to download prebuilt binaries
# - For CoreML acceleration: Use coreml-ep feature (macOS/iOS only)
ort = { version = "2.0.0-rc.11", default-features = false, features = ["ndarray", "std"] }
ndarray = "0.17"  # Used by ort for tensor operations (must match ort's ndarray version)
base64 = "0.21"  # For BPE token decoding
mel_spec = "0.2"  # For mel spectrogram computation (audio preprocessing)
num-complex = "0.4"  # Required by mel_spec for Complex numbers
rustfft = "6.2"  # For custom Whisper-compatible FFT (Slaney mel scale)
image = "0.25"  # For image resizing and processing
tokenizers = "0.19"  # For text tokenization (BERT, GPT, etc.)
hound = "3.5"  # For WAV file reading (audio preprocessing)
cmudict-fast = "0.8"  # For CMU pronouncing dictionary (TTS phonemization)
lazy_static = "1.4"  # For static CMU dictionary initialization
unicode-normalization = "0.1"  # For NFC normalization of IPA phonemes

# Kokoro TTS dependencies (v0.0.10)
# Note: espeak-ng phonemization now uses system command (no library linking required)
ndarray-npy = "0.10"  # For loading voice embeddings from npz files (must match ndarray version)
regex = "1.10"  # For text normalization patterns

# Candle - Pure Rust ML framework (optional, for Whisper ASR)
# Enable with: cargo build --features candle
candle-core = { version = "0.8", optional = true }
candle-nn = { version = "0.8", optional = true }
candle-transformers = { version = "0.8", optional = true }
safetensors = { version = "0.4", optional = true }
hf-hub = { version = "0.3", optional = true }  # For downloading models from HuggingFace
byteorder = { version = "1.5", optional = true }  # For reading mel filter bytes
num-traits = { version = "0.2", optional = true }  # Required by candle audio

# Mistral.rs - Pure Rust LLM inference (optional, for local LLMs)
# Enable with: cargo build --features local-llm
# For GPU: cargo build --features local-llm,local-llm-metal (macOS) or local-llm-cuda (NVIDIA)
mistralrs = { git = "https://github.com/EricLBuehler/mistral.rs.git", optional = true, default-features = false }

[features]
default = ["onnx-runtime", "ort-download"]
onnx-runtime = []
# Desktop/iOS/macOS: Download prebuilt ONNX Runtime binaries during build
# Note: tls-native required for ort 2.0.0-rc.11+ to download binaries
ort-download = ["ort/download-binaries", "ort/tls-native"]
# Android: Load ONNX Runtime dynamically at runtime (from AAR .so files)
# Note: When using ort-load-dynamic, don't enable ort-download
ort-load-dynamic = ["ort/load-dynamic"]
candle = ["dep:candle-core", "dep:candle-nn", "dep:candle-transformers", "dep:safetensors", "dep:hf-hub", "dep:byteorder", "dep:num-traits"]
candle-metal = ["candle", "candle-core/metal", "candle-nn/metal", ]
candle-cuda = ["candle", "candle-core/cuda"]
# CoreML Execution Provider for Apple Neural Engine acceleration (macOS/iOS)
# Enable with: cargo build --features coreml-ep
coreml-ep = ["ort/coreml"]
# Local LLM inference via mistral.rs (CPU by default)
# Enable with: cargo build --features local-llm
local-llm = ["dep:mistralrs"]
# Local LLM with Metal acceleration (macOS/iOS)
local-llm-metal = ["local-llm", "mistralrs/metal"]
# Local LLM with CUDA acceleration (NVIDIA GPUs)
local-llm-cuda = ["local-llm", "mistralrs/cuda"]

[dev-dependencies]
tempfile = "3.8"
xybrid-sdk = { path = "../sdk" }
dirs = "5.0"
filetime = "0.2"
hound = "3.5"  # WAV file reading for tests
walkdir = "2.4"  # For directory traversal in tests
ureq = { version = "2.8", default-features = false, features = ["json", "tls"] }  # For HTTP in tests
criterion = { version = "0.5", features = ["html_reports"] }  # Benchmarking framework

[[bench]]
name = "inference_backends"
harness = false

# Platform-specific ureq dependencies
# Non-Android: Use default ureq with TLS
[target.'cfg(not(target_os = "android"))'.dependencies]
ureq = { version = "2.8", default-features = false, features = ["json", "tls"] }

# Android: Use ureq with explicit rustls/ring (aws-lc-rs uses getentropy unavailable on API 24)
[target.'cfg(target_os = "android")'.dependencies]
ureq = { version = "2.8", default-features = false, features = ["json", "tls"] }
# Force rustls to use ring crypto backend instead of aws-lc-rs
rustls = { version = "0.23", default-features = false, features = ["ring", "std", "tls12"] }
