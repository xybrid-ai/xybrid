{
  "version": "2.0",
  "description": "Model manifest for xybrid integration tests - supports registry and direct URL downloads",
  "models": {
    "wav2vec2-base-960h": {
      "description": "Wav2Vec2 Base 960h - English ASR with CTC decoding",
      "source": "registry",
      "size_mb": 220,
      "task": "speech-recognition"
    },
    "kokoro-82m": {
      "description": "High-quality TTS model (82M params) with Misaki dictionary phonemizer",
      "source": "registry",
      "size_mb": 330,
      "task": "text-to-speech"
    },
    "whisper-tiny": {
      "description": "Whisper tiny for ASR (Candle/SafeTensors format)",
      "source": "registry",
      "size_mb": 150,
      "task": "speech-recognition"
    },
    "mnist": {
      "description": "Handwritten digit classification (28x28 grayscale, 10 classes)",
      "source": "url",
      "size_mb": 0.026,
      "task": "image-classification",
      "files": [
        {
          "url": "https://github.com/onnx/models/raw/main/validated/vision/classification/mnist/model/mnist-12.onnx",
          "output": "model.onnx"
        }
      ],
      "model_metadata": {
        "model_id": "mnist-digit-recognition",
        "version": "12",
        "description": "MNIST handwritten digit recognition model (opset 12). Input: [1, 1, 28, 28] grayscale image normalized to [0.0, 1.0]. Output: [1, 10] probabilities for digits 0-9.",
        "execution_template": {
          "type": "SimpleMode",
          "model_file": "model.onnx"
        },
        "preprocessing": [
          {
            "type": "Reshape",
            "shape": [1, 1, 28, 28]
          },
          {
            "type": "Normalize",
            "mean": [0.0],
            "std": [255.0]
          }
        ],
        "postprocessing": [
          {
            "type": "Softmax",
            "dim": 1
          }
        ],
        "files": ["model.onnx"],
        "metadata": {
          "input_shape": [1, 1, 28, 28],
          "output_shape": [1, 10],
          "opset_version": 12,
          "task": "image_classification",
          "num_classes": 10
        }
      }
    },
    "resnet50": {
      "description": "ImageNet classification (224x224 RGB, 1000 classes, 75.81% accuracy)",
      "source": "url",
      "size_mb": 98,
      "task": "image-classification",
      "files": [
        {
          "url": "https://github.com/onnx/models/raw/main/validated/vision/classification/resnet/model/resnet50-v2-7.onnx",
          "output": "resnet50-v2-7.onnx"
        }
      ],
      "model_metadata": {
        "model_id": "resnet50-imagenet",
        "version": "v2-7",
        "description": "ResNet-50 v2 image classification model trained on ImageNet. Input: [1, 3, 224, 224] RGB image with ImageNet normalization. Output: [1, 1000] probabilities for ImageNet classes.",
        "execution_template": {
          "type": "SimpleMode",
          "model_file": "resnet50-v2-7.onnx"
        },
        "preprocessing": [
          {
            "type": "Reshape",
            "shape": [1, 3, 256, 256]
          },
          {
            "type": "CenterCrop",
            "width": 224,
            "height": 224
          },
          {
            "type": "Normalize",
            "mean": [123.675, 116.28, 103.53],
            "std": [58.395, 57.12, 57.375]
          }
        ],
        "postprocessing": [
          {
            "type": "Softmax",
            "dim": 1
          },
          {
            "type": "TopK",
            "k": 5
          }
        ],
        "files": ["resnet50-v2-7.onnx"],
        "metadata": {
          "input_shape": [1, 3, 224, 224],
          "output_shape": [1, 1000],
          "opset_version": 7,
          "task": "image_classification",
          "num_classes": 1000,
          "dataset": "ImageNet",
          "accuracy": {
            "top1": 0.7581,
            "top5": 0.9282
          }
        }
      }
    },
    "mobilenet": {
      "description": "Lightweight ImageNet classification (224x224 RGB, 1000 classes)",
      "source": "url",
      "size_mb": 13.3,
      "task": "image-classification",
      "files": [
        {
          "url": "https://github.com/onnx/models/raw/main/validated/vision/classification/mobilenet/model/mobilenetv2-12.onnx",
          "output": "mobilenetv2-12.onnx"
        }
      ],
      "model_metadata": {
        "model_id": "mobilenetv2-imagenet",
        "version": "v2-12",
        "description": "MobileNetV2 lightweight image classification model trained on ImageNet. Input: [1, 3, 224, 224] RGB image normalized to [0, 1] with ImageNet mean/std. Output: [1, 1000] probabilities for ImageNet classes.",
        "execution_template": {
          "type": "SimpleMode",
          "model_file": "mobilenetv2-12.onnx"
        },
        "preprocessing": [
          {
            "type": "Reshape",
            "shape": [1, 3, 256, 256]
          },
          {
            "type": "CenterCrop",
            "width": 224,
            "height": 224
          },
          {
            "type": "Normalize",
            "mean": [0.485, 0.456, 0.406],
            "std": [0.229, 0.224, 0.225]
          }
        ],
        "postprocessing": [
          {
            "type": "Softmax",
            "dim": 1
          },
          {
            "type": "TopK",
            "k": 5
          }
        ],
        "files": ["mobilenetv2-12.onnx"],
        "metadata": {
          "input_shape": [1, 3, 224, 224],
          "output_shape": [1, 1000],
          "opset_version": 12,
          "task": "image_classification",
          "num_classes": 1000,
          "dataset": "ImageNet",
          "accuracy": {
            "top1": 0.6948
          },
          "model_size_mb": 13.3,
          "notes": "Lightweight mobile-optimized architecture. Normalize input to [0,1] then apply mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]"
        }
      }
    },
    "all-minilm": {
      "description": "Sentence embeddings (384-dim, text to vector)",
      "source": "url",
      "size_mb": 86,
      "task": "text-embedding",
      "files": [
        {
          "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/onnx/model.onnx",
          "output": "model.onnx"
        },
        {
          "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/vocab.txt",
          "output": "vocab.txt"
        },
        {
          "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer.json",
          "output": "tokenizer.json"
        },
        {
          "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json",
          "output": "config.json"
        }
      ],
      "model_metadata": {
        "model_id": "all-minilm",
        "version": "L6-v2",
        "execution_template": {
          "type": "SimpleMode",
          "model_file": "model.onnx"
        },
        "preprocessing": [
          {
            "type": "Tokenize",
            "vocab_file": "tokenizer.json",
            "tokenizer_type": "WordPiece",
            "max_length": 512
          }
        ],
        "postprocessing": [
          {
            "type": "MeanPool",
            "dim": 1
          }
        ],
        "files": [
          "model.onnx",
          "vocab.txt",
          "tokenizer.json",
          "config.json"
        ],
        "metadata": {
          "task": "text-embedding",
          "embedding_dim": 384
        }
      }
    },
    "distilbert": {
      "description": "DistilBERT sentence embeddings (384-dim, same architecture as all-minilm)",
      "source": "url",
      "size_mb": 86,
      "task": "text-embedding",
      "files": [
        {
          "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/onnx/model.onnx",
          "output": "model.onnx"
        },
        {
          "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/vocab.txt",
          "output": "vocab.txt"
        },
        {
          "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer.json",
          "output": "tokenizer.json"
        },
        {
          "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json",
          "output": "config.json"
        }
      ],
      "model_metadata": {
        "model_id": "all-minilm-l6-v2",
        "version": "1.0",
        "description": "Sentence transformer model for generating embeddings (384 dimensions)",
        "execution_template": {
          "type": "SimpleMode",
          "model_file": "model.onnx"
        },
        "preprocessing": [
          {
            "type": "Tokenize",
            "vocab_file": "tokenizer.json",
            "tokenizer_type": "WordPiece",
            "max_length": 512
          }
        ],
        "postprocessing": [
          {
            "type": "MeanPool",
            "dim": 1
          }
        ],
        "files": [
          "model.onnx",
          "tokenizer.json",
          "vocab.txt",
          "config.json"
        ],
        "metadata": {
          "architecture": "BertModel",
          "hidden_size": 384,
          "num_layers": 6,
          "num_attention_heads": 12,
          "max_position_embeddings": 512,
          "vocab_size": 30522
        }
      }
    },
    "kitten-tts": {
      "description": "Ultra-lightweight TTS (15M params, 8 voices, 24kHz audio, requires espeak-ng)",
      "source": "url",
      "size_mb": 24,
      "task": "text-to-speech",
      "archive": true,
      "archive_url": "https://github.com/k2-fsa/sherpa-onnx/releases/download/tts-models/kitten-nano-en-v0_1-fp16.tar.bz2",
      "archive_strip": "kitten-nano-en-v0_1-fp16",
      "model_metadata": {
        "model_id": "kitten-tts-nano",
        "version": "0.1",
        "description": "KittenTTS Nano - Ultra-lightweight TTS model (15M parameters, 24MB)",
        "execution_template": {
          "type": "SimpleMode",
          "model_file": "model.fp16.onnx"
        },
        "preprocessing": [
          {
            "type": "Phonemize",
            "tokens_file": "tokens.txt",
            "dict_file": "cmudict.dict",
            "add_padding": true
          }
        ],
        "postprocessing": [
          {
            "type": "TTSAudioEncode",
            "sample_rate": 24000,
            "apply_postprocessing": true
          }
        ],
        "files": [
          "model.fp16.onnx",
          "tokens.txt",
          "voices.bin"
        ],
        "metadata": {
          "task": "text-to-speech",
          "language": "en",
          "sample_rate": 24000,
          "num_voices": 8,
          "voice_names": [
            "expr-voice-2-m",
            "expr-voice-2-f",
            "expr-voice-3-m",
            "expr-voice-3-f",
            "expr-voice-4-m",
            "expr-voice-4-f",
            "expr-voice-5-m",
            "expr-voice-5-f"
          ],
          "license": "Apache-2.0",
          "source": "https://github.com/KittenML/KittenTTS",
          "notes": "Requires espeak-ng for phonemization. Install: brew install espeak-ng (macOS) or apt install espeak-ng (Linux)"
        }
      }
    },
    "qwen2.5-0.5b-instruct": {
      "description": "Qwen 2.5 0.5B Instruct - Small but capable instruction-tuned LLM (GGUF Q4_K_M)",
      "source": "url",
      "size_mb": 468,
      "task": "text-generation",
      "feature_gate": "local-llm",
      "files": [
        {
          "url": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_k_m.gguf",
          "output": "qwen2.5-0.5b-instruct-q4_k_m.gguf"
        }
      ],
      "model_metadata": {
        "model_id": "qwen2.5-0.5b-instruct",
        "version": "1.0",
        "description": "Qwen 2.5 0.5B Instruct - Small but capable instruction-tuned LLM from Alibaba Cloud",
        "execution_template": {
          "type": "Gguf",
          "model_file": "qwen2.5-0.5b-instruct-q4_k_m.gguf",
          "context_length": 4096
        },
        "preprocessing": [],
        "postprocessing": [],
        "files": [
          "qwen2.5-0.5b-instruct-q4_k_m.gguf",
          "model_metadata.json"
        ],
        "metadata": {
          "task": "text-generation",
          "architecture": "qwen2",
          "parameters": "0.5B",
          "quantization": "Q4_K_M",
          "license": "Apache-2.0",
          "languages": ["en", "zh"],
          "source": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF",
          "notes": "Requires --features local-llm to run. CPU release: ~10 tok/s. Metal has dispatch overhead for this small model."
        }
      }
    }
  }
}
